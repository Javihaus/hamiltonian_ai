{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install skorch"
      ],
      "metadata": {
        "id": "2CGiYsffJCIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC0tLTzZAREp"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from skorch import NeuralNetClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        "    SubsetRandomSampler,\n",
        "    TensorDataset\n",
        "    )\n",
        "from torch.optim import Optimizer\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    KFold,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV\n",
        "  )\n",
        "from sklearn.metrics import(\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score\n",
        "    )\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class MortgageDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Custom Optimizer\n",
        "class AdvancedSymplecticOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-2, beta=0.9, epsilon=1e-8):\n",
        "        defaults = dict(lr=lr, beta=beta, epsilon=epsilon)\n",
        "        super(AdvancedSymplecticOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['momentum'] = torch.zeros_like(p.data)\n",
        "\n",
        "                momentum = state['momentum']\n",
        "                lr, beta, eps = group['lr'], group['beta'], group['epsilon']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Implement 4th order symplectic integrator (Forest-Ruth algorithm)\n",
        "                momentum.mul_(beta).add_(grad, alpha=1 - beta)\n",
        "\n",
        "                # Compute adaptive step size\n",
        "                kinetic = 0.5 * (momentum ** 2).sum()\n",
        "                potential = 0.5 * (grad ** 2).sum()\n",
        "                hamiltonian = kinetic + potential\n",
        "                step_size = lr / (hamiltonian.sqrt() + eps)\n",
        "\n",
        "                p.add_(momentum, alpha=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class HamiltonianNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[512, 268], activation='leaky_relu', dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(nn.Linear(hidden_dims[-1], 2))  # Binary classification\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:  # All layers except the last\n",
        "            x = self.dropout(self.activation(layer(x)))\n",
        "        return self.layers[-1](x)  # Last layer (no activation for output)\n",
        "\n",
        "\n",
        "# Hamiltonian-inspired loss function\n",
        "def hamiltonian_loss(outputs, labels, model, reg_coeff=0.01):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    base_loss = loss_fct(outputs, labels)\n",
        "    # Add regularization based on Hamiltonian principles\n",
        "    param_norm = sum(p.norm().item() for p in model.parameters())\n",
        "    reg_term = reg_coeff * param_norm  # Use reg_coeff instead of a fixed value\n",
        "    return base_loss + reg_term\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc"
      ],
      "metadata": {
        "id": "4_J_-932AUAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv(\"Training.csv\")\n",
        "oot_test_df = pd.read_csv(\"OOT test.csv\")\n",
        "train_df = train_df.dropna()\n",
        "oot_test_df = oot_test_df.dropna()\n",
        "\n",
        "# Prepare features and target\n",
        "features = ['Credit_Score', 'Mortgage_Insurance', 'Number_of_units', 'CLoan_to_value',\n",
        "            'Debt_to_income', 'OLoan_to_value', 'Single_borrower']\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['DFlag']\n",
        "X_oot_test = oot_test_df[features]\n",
        "y_oot_test = oot_test_df['DFlag']\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_oot_test_scaled = scaler.transform(X_oot_test)\n",
        "\n",
        "# Apply SMOTE to balance the training dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_resampled)\n",
        "y_train_tensor = torch.LongTensor(y_train_resampled)\n",
        "X_oot_test_tensor = torch.FloatTensor(X_oot_test_scaled)\n",
        "y_oot_test_tensor = torch.LongTensor(y_oot_test.values)\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Create DataLoader for OOT testing\n",
        "oot_test_dataset = TensorDataset(X_oot_test_tensor, y_oot_test_tensor)\n",
        "oot_test_loader = DataLoader(oot_test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Print class distribution before and after SMOTE\n",
        "print(\"Class distribution before SMOTE:\", np.bincount(y_train))\n",
        "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))"
      ],
      "metadata": {
        "id": "6ZX_UzH8Njfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "I9mniCB9LqgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom LR Scheduler\n",
        "class CustomLRScheduler:\n",
        "    def __init__(self, optimizer, factor=0.5, patience=3, min_lr=1e-5):\n",
        "        self.optimizer = optimizer\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.best_loss = float('inf')\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        if self.bad_epochs > self.patience:\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = max(param_group['lr'] * self.factor, self.min_lr)\n",
        "            self.bad_epochs = 0\n",
        "            print(f\"Learning rate reduced to {param_group['lr']}\")"
      ],
      "metadata": {
        "id": "s8fIZy_T-k45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HamiltonianNNWrapper(BaseEstimator):\n",
        "    def __init__(self, hidden_dims=[64, 32], activation='relu', dropout_rate=0.2, lr=0.01, reg_coeff=0.01):\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.activation = activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.lr = lr\n",
        "        self.reg_coeff = reg_coeff\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        try:\n",
        "            X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "            y = torch.tensor(y, dtype=torch.long).to(self.device)\n",
        "\n",
        "            self.model = HamiltonianNN(\n",
        "                input_dim=X.shape[1],\n",
        "                hidden_dims=self.hidden_dims,\n",
        "                activation=self.activation,\n",
        "                dropout_rate=self.dropout_rate\n",
        "            ).to(self.device)\n",
        "\n",
        "            optimizer = AdvancedSymplecticOptimizer(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "            dataset = TensorDataset(X, y)\n",
        "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "            for epoch in range(10):  # You can adjust the number of epochs\n",
        "                for features, labels in loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = self.model(features)\n",
        "                    loss = hamiltonian_loss(outputs, labels, self.model, self.reg_coeff)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            return self\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fitting: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "        return predictions.cpu().numpy()"
      ],
      "metadata": {
        "id": "oliTfjovKcXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN hyperparameters optimization"
      ],
      "metadata": {
        "id": "nX5GcLsyOHMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"class HamiltonianNNWrapper(BaseEstimator):\n",
        "    def __init__(self, hidden_dims=[64, 32], activation='relu', dropout_rate=0.2, lr=1e-3, reg_coeff=0.01):\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.activation = activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.lr = lr\n",
        "        self.reg_coeff = reg_coeff\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y = torch.tensor(y, dtype=torch.long).to(self.device)\n",
        "\n",
        "        self.model = HamiltonianNN(\n",
        "            input_dim=X.shape[1],\n",
        "            hidden_dims=self.hidden_dims,\n",
        "            activation=self.activation,\n",
        "            dropout_rate=self.dropout_rate\n",
        "        ).to(self.device)\n",
        "\n",
        "        optimizer = AdvancedSymplecticOptimizer(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        dataset = TensorDataset(X, y)\n",
        "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        for epoch in range(10):  # You can adjust the number of epochs\n",
        "            for features, labels in loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(features)\n",
        "                loss = hamiltonian_loss(outputs, labels, self.model, self.reg_coeff)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "# Hyperparameter search space\n",
        "param_dist = {\n",
        "    'hidden_dims': [[64, 32], [128, 64]],\n",
        "    'activation': ['relu', 'leaky_relu'],\n",
        "    'dropout_rate': [0.2, 0.4],\n",
        "    'lr': [1e-3, 1e-2],\n",
        "    'reg_coeff': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Randomized search\n",
        "search = RandomizedSearchCV(\n",
        "    HamiltonianNNWrapper(),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Perform the search\n",
        "search.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Print best parameters and score\n",
        "print(\"Best parameters:\", search.best_params_)\n",
        "print(\"Best F1 score:\", search.best_score_)\n",
        "\n",
        "# Train final model with best hyperparameters\n",
        "best_model = search.best_estimator_\"\"\""
      ],
      "metadata": {
        "id": "rVV7R5RJKwUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the mode with the best hyperparameters\n",
        "Best parameters:\n",
        "\n",
        "*   reg_coeff = 0.01,\n",
        "*   lr = 0.01\n",
        "*   hidden_dims = [128, 64]\n",
        "*   dropout_rate = 0.2\n",
        "*   activation = 'leaky_relu'"
      ],
      "metadata": {
        "id": "gB5-QU2bONl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train final model with best hyperparameters\n",
        "num_epochs = 30\n",
        "model = HamiltonianNN(\n",
        "    input_dim=X_train.shape[1],\n",
        "    hidden_dims=[512, 256], #[128, 64]\n",
        "    activation='leaky_relu',\n",
        "    dropout_rate=0.2\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# K-fold Cross-validation\n",
        "k_folds = 3\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "cv_accuracies = []\n",
        "cv_f1_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_tensor)):\n",
        "    print(f\"\\nFold {fold+1}/{k_folds}\")\n",
        "\n",
        "    X_train_fold, X_val_fold = X_train_tensor[train_idx], X_train_tensor[val_idx]\n",
        "    y_train_fold, y_val_fold = y_train_tensor[train_idx], y_train_tensor[val_idx]\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
        "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    model = HamiltonianNN(input_dim=X_train.shape[1]).to(device)\n",
        "    optimizer = AdvancedSymplecticOptimizer(model.parameters(), lr=0.01)\n",
        "    scheduler = CustomLRScheduler(optimizer)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(features)\n",
        "            loss = hamiltonian_loss(outputs, labels, model)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                val_loss += hamiltonian_loss(outputs, labels, model).item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy, val_precision, val_recall, val_f1, val_auc = evaluate_model(model, val_loader, device)\n",
        "        print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}, Precision: {val_precision:.4f},  Recall: {val_recall:.4f}\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(\"New best model saved\")\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    model.load_state_dict(best_model_state)\n",
        "    val_accuracy, val_precision, val_recall, val_f1, val_auc = evaluate_model(model, val_loader, device)\n",
        "    cv_accuracies.append(val_accuracy)\n",
        "    cv_f1_scores.append(val_f1)\n",
        "    print(f\"Final Validation - Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "# Load best model for final evaluation\n",
        "model.load_state_dict(best_model_state)\n",
        "print(\"Training completed. Best model loaded.\")\n",
        "\n",
        "val_accuracy, val_precision, val_recall, val_f1, val_auc = evaluate_model(model, val_loader, device)\n",
        "cv_accuracies.append(val_accuracy)\n",
        "cv_f1_scores.append(val_f1)\n",
        "print(f\"Final Validation - Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")"
      ],
      "metadata": {
        "id": "JBkQZFcALvh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCross-validation results:\")\n",
        "print(f\"Mean Accuracy: {np.mean(cv_accuracies):.4f} (+/- {np.std(cv_accuracies):.4f})\")\n",
        "print(f\"Mean F1 Score: {np.mean(cv_f1_scores):.4f} (+/- {np.std(cv_f1_scores):.4f})\")\n",
        "print(f\"Mean AUC Score: {np.mean(val_auc):.4f} (+/- {np.std(val_auc):.4f})\")\n",
        "print(f\"Mean Precision: {np.mean(val_precision):.4f} (+/- {np.std(val_precision):.4f})\")\n",
        "print(f\"Mean Recall: {np.mean(val_recall):.4f} (+/- {np.std(val_recall):.4f})\")"
      ],
      "metadata": {
        "id": "_5U_l_-1L0br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation results:\n",
        "- Mean Accuracy: 0.6975 (+/- 0.0003)\n",
        "- Mean F1 Score: 0.6968 (+/- 0.0003)\n",
        "- Mean AUC Score: 0.6974 (+/- 0.0000)\n",
        "- Mean Precision: 0.6994 (+/- 0.0000)\n",
        "- Mean Recall: 0.6973 (+/- 0.0000)"
      ],
      "metadata": {
        "id": "cAWQDG7einx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using XGBoost"
      ],
      "metadata": {
        "id": "-jtjM-2BuSoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled training data into training and validation sets\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train_resampled, y_train_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [5, 7],\n",
        "    'learning_rate': [0.1, 0.3],\n",
        "    'n_estimators': [200, 300],\n",
        "    'min_child_weight': [1, 3]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           cv=3, scoring='roc_auc', verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Train the best model on the entire training set\n",
        "best_xgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the OOT test set\n",
        "y_oot_pred = best_xgb_model.predict(X_oot_test_scaled)\n",
        "y_oot_pred_proba = best_xgb_model.predict_proba(X_oot_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_oot_test, y_oot_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_oot_test, y_oot_pred, average='weighted', zero_division=0)\n",
        "auc = roc_auc_score(y_oot_test, y_oot_pred_proba)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nXGBoost Final OOT Test Results:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# Print best parameters\n",
        "print(\"\\nBest XGBoost Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = best_xgb_model.feature_importances_\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos = np.arange(sorted_idx.shape[0]) + .5"
      ],
      "metadata": {
        "id": "0xS_BTLnuRw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Final OOT Test Results:\n",
        "- Accuracy: 0.9310\n",
        "- Precision: 0.9796\n",
        "- Recall: 0.9310\n",
        "-F1 Score: 0.9542\n",
        "- AUC: 0.6666\n",
        "- Best XGBoost Parameters: {'learning_rate': 0.3, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 300}\"."
      ],
      "metadata": {
        "id": "dDN3_ksOIEOZ"
      }
    }
  ]
}