{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Optimizing AI Reasoning: A Hamiltonian Dynamics Approach to Multi-Hop Question Answering\n",
        "\n",
        "Author: Javier Marín\n",
        "Email: javier@jmarin.info\n",
        "Version: 1.0\n",
        "Last Updated: March 8, 2024\n",
        "\n",
        "Copyright (c) 2024 Javier Marín\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "Description:\n",
        "This notebook implements a novel approach to multi-hop reasoning in AI systems\n",
        "using principles from Hamiltonian mechanics. It includes a custom optimizer\n",
        "(AdvancedSymplecticOptimizer) and a modified GPT-2 model (HamiltonianGPT2)\n",
        "for improved performance on multi-hop question answering tasks.\n",
        "\n",
        "Requirements:\n",
        "- Python 3.7+\n",
        "- PyTorch 1.7+\n",
        "- Transformers 4.0+\n",
        "- NumPy\n",
        "- Pandas\n",
        "- Scikit-learn\n",
        "- TQDM\n",
        "\n",
        "For full requirements, see requirements.txt\n",
        "\n",
        "Usage:\n",
        "This notebook is designed to be run in a Jupyter environment or Google Colab.\n",
        "Ensure all required libraries are installed and the dataset (obqa_chains.csv)\n",
        "is in the same directory as this notebook.\n",
        "\n",
        "For more information, please refer to the accompanying paper:\n",
        "[https://arxiv.org/abs/2410.04415]\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EP-sDRXJuOqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\textbf{Optimizing AI Reasoning: A Hamiltonian Dynamics Approach to Multi-Hop Question Answering}$\n",
        "\n",
        "This notebook implements the concepts and methods described in our paper \"Optimizing AI Reasoning: A Hamiltonian Dynamics Approach to Multi-Hop Question Answering\" (https://arxiv.org/abs/2410.04415).\n"
      ],
      "metadata": {
        "id": "zOtsls4NxvvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Introduction\n",
        "\n",
        "In this work, we propose a novel approach to analyzing and improving multi-hop reasoning in AI systems by drawing inspiration from Hamiltonian mechanics. Our method maps reasoning chains in embedding spaces to Hamiltonian systems, allowing us to leverage powerful analytical tools from classical physics."
      ],
      "metadata": {
        "id": "FaEgpJi7zZxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Theoretical Background\n"
      ],
      "metadata": {
        "id": "ReIRv_3vznfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Hamiltonian Systems\n",
        "The Hamiltonian formalism provides an effective mathematical framework for developing conservative mechanical system theory and is a geometric language for multiple fields of physics. A Hamiltonian can be defined with the following 2n ordinary differential equations (Easton, 1993):\n",
        "\n",
        "<center>$\\dot{q}\\ =H_p$</center>\n",
        "\n",
        "<center>$\\dot{p}=-H_q$</center>\n",
        "\n",
        "<center>$\\dot{q_l}\\ =\\ \\frac{\\partial H}{\\partial p_l}(t,q,p)\\ \\ ,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\dot{p_i}\\ =\\ -\\frac{\\partial H}{\\partial q_i}(t,p,q)\\ \\ \\ \\ \\ \\ \\ \\ \\\n",
        "$</center>\n",
        "\n",
        "where $H\\ =\\ H(t,q,p)$ is the Hamiltonian, $q$ and $p$ are the position and momentum vectors of a mechanical system with $n$ degrees of freedom, and $t$ is the time. For the purpose of this experiment, we can define the Hamiltonian, $H$, of a system as (Marin, 2024):\n",
        "\n",
        "<center>$H(q, p) = T(p) + V(q)$</center>\n",
        "\n",
        "where position and momentum vectors of a mechanical system with $n$ degrees of freedom, $T(p)$ is the kinetic energy, and $V(q$) is the potential energy of the system. Each point in phase space represents a unique state of the system, defined by its position and momentum coordinates $(q,p)$.\n",
        "\n",
        "In our AI context:\n",
        "- $q$ represents the current state of reasoning\n",
        "- $p$ represents the change in reasoning\n",
        "- $T(p)$ represents the \"cost\" of changing the reasoning state\n",
        "- $V(q)$ represents the relevance or correctness of the current reasoning state\n",
        "\n",
        "$\\textbf{References}$\n",
        "- Marin, J. (2024). Optimizing AI Reasoning: A Hamiltonian Dynamics Approach to Multi-Hop Question Answering. ArXiv Preprint ArXiv:2410.04415.\n",
        "- Easton, R. W. (1993). Introduction to Hamiltonian dynamical systems and the N-body problem (KR Meyer and GR Hall). SIAM Review, 35(4), 659."
      ],
      "metadata": {
        "id": "qWfo-soSzuhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Symplectic Integration\n",
        "\n",
        "Symplectic structures are fundamental geometric objects in differential geometry and classical mechanics (Goldman, 1984). The phase space of a Hamiltonian system is a symplectic manifold, and Hamiltonian flows preserve the symplectic structure (Prugovečki, 1979). This objects provide a framework for understanding the relationship between position and momentum in physical systems, allowing for the formulation of Hamilton's equations of motion. Simply said, symplectic structures are specific rules that define how things move in physics, similar to an equation for motion. They help us grasp how items' positions and velocities are related, allowing us to predict how things will behave over time.\n",
        "\n",
        "The kinetic term $T(p)$ can be interpreted as the cognitive effort or computational cost associated with changing the reasoning state (Friston, 2010)\n",
        "<center/>$T(p)=\\ \\frac{1}{2}p^2$</center>\n",
        "\n",
        "where $p$ is the magnitude of the change vector. This quadratic form is analogous to kinetic energy in classical physics and penalizes large, rapid changes in reasoning. The term $V(q)$ represents the degree to which the present reasoning state corresponds with the objective or question being addressed (Marin, 2024). The term $V(q)$ represents the degree to which the present reasoning state corresponds with the objective or question being addressed and can be defined as $\\frac{1}{2}|q|$. Then the exact Hamiltonian equation is:\n",
        "\n",
        "<center>$H_0(p,q)=\\frac{1}{2}p^2 - \\frac{1}{2}|q|$</center>\n",
        "\n",
        "We use Forest-Ruth algorithm (Omelyan et al., 2002), a 4th order symplectic integrator in our optimizer. A symmetric nth order symplectic algorithm advances this system temporally with Hamiltonian (Chin et al, 2018).\n",
        "<center>$H(p, q) = H_0(p, q) + \\epsilon^nH_n(p, q) + O(\\epsilon^{n+2})$</center>\n",
        "\n",
        "witch adds the error term $\\epsilon^nH_n(p, q)$ to the exact Hamiltonian equation mentioned before. This is the error term introduced by the numerical method, while $O(\\epsilon^{n+2})$ represents higher-order error terms.\n",
        "\n",
        "\n",
        "\n",
        "$\\textbf{References}$\n",
        " - Goldman, W. M. (1984). The symplectic nature of fundamental groups of surfaces. Advances in Mathematics, 54(2), 200–225.\n",
        " - Prugovečki, E. (1979). Stochastic phase spaces and master Liouville spaces in statistical mechanics. Foundations of Physics, 9(7–8), 575–587.\n",
        " - Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2), 127–138.\n",
        " - Marin, J. (2024). Optimizing AI Reasoning: A Hamiltonian Dynamics Approach to Multi-Hop Question Answering. ArXiv Preprint ArXiv:2410.04415.\n",
        " - Omelyan, I. P., Mryglod, I. M., & Folk, R. (2002). Optimized Forest–Ruth- and Suzuki-like algorithms for integration of motion in many-body systems. Computer Physics Communications, 146(2), 188–202. doi:10.1016/s0010-4655(02)00451-4\n",
        " - Chin, S. A., & Kidwell, D. W. (2000). Higher-order force gradient symplectic algorithms. Physical Review E, 62(6), 8746."
      ],
      "metadata": {
        "id": "Ygn8_xCdz12G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Implementation"
      ],
      "metadata": {
        "id": "pZZDMS-_0CZE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd6XyAChnU1-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torch.optim import Optimizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Custome functions"
      ],
      "metadata": {
        "id": "TRDe-LjGxyqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class OBQADataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.inputs = tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "            )\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.inputs.items()}, self.labels[idx]\n",
        "\n",
        "# Custom Optimizer\n",
        "class AdvancedSymplecticOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, beta=0.9, epsilon=1e-8):\n",
        "        defaults = dict(lr=lr, beta=beta, epsilon=epsilon)\n",
        "        super(AdvancedSymplecticOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['momentum'] = torch.zeros_like(p.data)\n",
        "\n",
        "                momentum = state['momentum']\n",
        "                lr, beta, eps = group['lr'], group['beta'], group['epsilon']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Implement 4th order symplectic integrator (Forest-Ruth algorithm)\n",
        "                momentum.mul_(beta).add_(grad, alpha=1 - beta)\n",
        "\n",
        "                # Compute adaptive step size\n",
        "                kinetic = 0.5 * (momentum ** 2).sum()\n",
        "                potential = 0.5 * (grad ** 2).sum()\n",
        "                hamiltonian = kinetic + potential\n",
        "                step_size = lr / (hamiltonian.sqrt() + eps)\n",
        "\n",
        "                p.add_(momentum, alpha=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\"\"\"\n",
        "Modified GPT-2 Model\n",
        "Using GPT-2 as a base model and adding a classification layer on top for\n",
        "our specific task, which is a common and valid approach in transfer learning.\n",
        "We've added a custom classification layer to the GPT-2 model\n",
        "This layer doesn't exist in the original GPT-2 model, so it can't be\n",
        "initialized with pre-trained weights. We are  fine-tuning the model\n",
        " (including the new classification layer) on our specific task.\n",
        " \"\"\"\n",
        "class HamiltonianGPT2(GPT2LMHeadModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = nn.Linear(config.n_embd, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = super().forward(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "            )\n",
        "        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n",
        "        pooled_output = hidden_states.mean(dim=1)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (logits, loss)\n",
        "\n",
        "\n",
        "# Hamiltonian-inspired loss function\n",
        "def hamiltonian_loss(outputs, labels, model):\n",
        "    logits, base_loss = outputs\n",
        "    if base_loss is None:\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        base_loss = loss_fct(logits, labels)\n",
        "    # Add regularization based on Hamiltonian principles\n",
        "    param_norm = sum(p.norm().item() for p in model.parameters())\n",
        "    reg_term = 0.01 * param_norm  # Adjust coefficient as needed\n",
        "    return base_loss + reg_term\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch[0].items()}\n",
        "            labels = batch[1].to(device)\n",
        "\n",
        "            if isinstance(model, HamiltonianGPT2):\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs[0]\n",
        "            else:\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits.mean(dim=1)  # Average over sequence length\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        average='weighted',\n",
        "        zero_division=0\n",
        "        )\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "LIHFHrTEt-jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Model initialization"
      ],
      "metadata": {
        "id": "1pnYQcXOx44y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the model initialization\n",
        "config = GPT2Config.from_pretrained('gpt2', output_hidden_states=True)\n",
        "model = HamiltonianGPT2.from_pretrained('gpt2', config=config)\n",
        "print(\"Note: The classifier layer has been newly initialized and will be trained on our specific task.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUfDMTUlsXY3",
        "outputId": "1e068e23-dadb-4313-9c2f-69903b3bf677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "Some weights of HamiltonianGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: The classifier layer has been newly initialized and will be trained on our specific task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Load Data"
      ],
      "metadata": {
        "id": "-9ilX5eEx-uM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the OpenBookQA (OBQA) dataset for our research, which provides a standard to assess the question answering and reasoning abilities of AI systems. The OBQA dataset was presented by Mihaylov et al. (2018) in their research on open-book question answering. Our experiment on explanation generation concentrates on the OBQA test set, which has 500 questions.\n",
        "\n",
        "###References\n",
        "- Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013b). Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems, 26."
      ],
      "metadata": {
        "id": "3zOCG4ytRA_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "df = pd.read_csv(\"obqa_chains.csv\", sep=\";\")\n",
        "texts = df['Fact1'] + ' ' + df['Fact2']\n",
        "labels = df['Turk'].apply(lambda x: 1 if 'yes' in str(x).lower() else 0)\n",
        "\n",
        "# Split data into train+val and test sets\n",
        "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        "    )"
      ],
      "metadata": {
        "id": "3nC26qK9uDZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Initialize model"
      ],
      "metadata": {
        "id": "e6uqYEetyB7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Prepare datasets and dataloaders\n",
        "train_val_dataset = OBQADataset(\n",
        "    train_val_texts.tolist(),\n",
        "    train_val_labels.tolist(),\n",
        "    tokenizer, max_length=128\n",
        "    )\n",
        "test_dataset = OBQADataset(\n",
        "    test_texts.tolist(),\n",
        "    test_labels.tolist(),\n",
        "    tokenizer, max_length=128\n",
        "    )\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training setup\n",
        "config = GPT2Config.from_pretrained('gpt2', output_hidden_states=True)\n",
        "model = HamiltonianGPT2.from_pretrained('gpt2', config=config).to(device)\n",
        "print(\"Note: Some weights are newly initialized as expected for the classifier layer.\")\n",
        "\n",
        "optimizer = AdvancedSymplecticOptimizer(model.parameters(), lr=5e-5)\n",
        "num_epochs = 5\n",
        "\n",
        "# K-fold Cross-validation\n",
        "k_folds = 3\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store results\n",
        "cv_accuracies = []\n",
        "cv_f1_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_dataset)):\n",
        "    print(f\"\\nFold {fold+1}/{k_folds}\")\n",
        "\n",
        "    # Create data loaders for train and validation sets\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_val_dataset,\n",
        "        batch_size=16,\n",
        "        sampler=train_sampler\n",
        "        )\n",
        "    val_loader = DataLoader(\n",
        "        train_val_dataset,\n",
        "        batch_size=16,\n",
        "        sampler=val_sampler\n",
        "        )\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    config = GPT2Config.from_pretrained('gpt2', output_hidden_states=True)\n",
        "    model = HamiltonianGPT2.from_pretrained('gpt2', config=config).to(device)\n",
        "    optimizer = AdvancedSymplecticOptimizer(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            inputs = {k: v.to(device) for k, v in batch[0].items()}\n",
        "            labels = batch[1].to(device)\n",
        "\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = hamiltonian_loss(outputs, labels, model)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validate after each epoch\n",
        "        val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(\n",
        "            model,\n",
        "            val_loader,\n",
        "            device\n",
        "            )\n",
        "        print(f\"Validation - Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Final validation\n",
        "    val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(\n",
        "        model,\n",
        "        val_loader,\n",
        "        device\n",
        "        )\n",
        "    cv_accuracies.append(val_accuracy)\n",
        "    cv_f1_scores.append(val_f1)\n",
        "    print(f\"Final Validation - Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"\\nCross-validation results:\")\n",
        "print(f\"Mean Accuracy: {np.mean(cv_accuracies):.4f} (+/- {np.std(cv_accuracies):.4f})\")\n",
        "print(f\"Mean F1 Score: {np.mean(cv_f1_scores):.4f} (+/- {np.std(cv_f1_scores):.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTeSsOpcuPUM",
        "outputId": "73ba1e6d-66fc-43a7-e74c-c2917f8a233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of HamiltonianGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: Some weights are newly initialized as expected for the classifier layer.\n",
            "\n",
            "Fold 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of HamiltonianGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/5: 100%|██████████| 34/34 [00:10<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Train Loss: 97.1495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.0865, F1: 0.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 34/34 [00:07<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Train Loss: 96.6095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.1316, F1: 0.1003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 34/34 [00:07<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Average Train Loss: 96.1578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.4737, F1: 0.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 34/34 [00:07<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Average Train Loss: 95.8661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8684, F1: 0.8611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 34/34 [00:07<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Average Train Loss: 95.7034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9098, F1: 0.8704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.54it/s]\n",
            "Some weights of HamiltonianGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation - Accuracy: 0.9098, F1: 0.8704\n",
            "\n",
            "Fold 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 34/34 [00:07<00:00,  4.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Train Loss: 96.9004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9323, F1: 0.8997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 34/34 [00:07<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Train Loss: 96.7498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9323, F1: 0.8997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 34/34 [00:07<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Average Train Loss: 96.5963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9323, F1: 0.8997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 34/34 [00:07<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Average Train Loss: 96.4597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9323, F1: 0.8997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 34/34 [00:07<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Average Train Loss: 96.3228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.9323, F1: 0.8997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 14.39it/s]\n",
            "Some weights of HamiltonianGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation - Accuracy: 0.9323, F1: 0.8997\n",
            "\n",
            "Fold 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 34/34 [00:07<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Train Loss: 96.8905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.1241, F1: 0.0274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 34/34 [00:07<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Train Loss: 96.4422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.1165, F1: 0.0259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 34/34 [00:07<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Average Train Loss: 96.0641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.4436, F1: 0.5358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 34/34 [00:07<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Average Train Loss: 95.7965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8571, F1: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 34/34 [00:08<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Average Train Loss: 95.6512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.8759, F1: 0.8180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 17/17 [00:01<00:00, 15.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation - Accuracy: 0.8759, F1: 0.8180\n",
            "\n",
            "Cross-validation results:\n",
            "Mean Accuracy: 0.9060 (+/- 0.0232)\n",
            "Mean F1 Score: 0.8627 (+/- 0.0338)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on full training set\n",
        "full_train_loader = DataLoader(train_val_dataset, batch_size=16, shuffle=True)\n",
        "model = HamiltonianGPT2.from_pretrained('gpt2', config=config).to(device)\n",
        "optimizer = AdvancedSymplecticOptimizer(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(full_train_loader, desc=f\"Full Training - Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = hamiltonian_loss(outputs, labels, model)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(full_train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMh9GEGI3_l9",
        "outputId": "02d119d6-a899-495d-a9bd-217efe96220c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of HamiltonianGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Full Training - Epoch 1/5: 100%|██████████| 50/50 [00:10<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Train Loss: 96.3875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Full Training - Epoch 2/5: 100%|██████████| 50/50 [00:10<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Train Loss: 95.9207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Full Training - Epoch 3/5: 100%|██████████| 50/50 [00:10<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Average Train Loss: 95.6713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Full Training - Epoch 4/5: 100%|██████████| 50/50 [00:10<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Average Train Loss: 95.6040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Full Training - Epoch 5/5: 100%|██████████| 50/50 [00:10<00:00,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Average Train Loss: 95.6056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Results and Analysis"
      ],
      "metadata": {
        "id": "3gx2h1znyHVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Evaluate standard model"
      ],
      "metadata": {
        "id": "pmcGbQQa638p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate standard GPT-2\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "print(\"\\nEvaluating Standard GPT-2:\")\n",
        "standard_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "standard_accuracy, standard_precision, standard_recall, standard_f1 = evaluate_model(standard_gpt2, test_loader, device)\n",
        "\n",
        "print(\"Standard GPT-2 Results:\")\n",
        "print(f\"Accuracy: {standard_accuracy:.4f}, Precision: {standard_precision:.4f}, Recall: {standard_recall:.4f}, F1: {standard_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbqoIuDm7Cd2",
        "outputId": "c9f1f994-1e92-477a-dcea-24a8841fd476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Standard GPT-2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 13/13 [00:00<00:00, 17.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard GPT-2 Results:\n",
            "Accuracy: 0.0200, Precision: 0.0105, Recall: 0.0200, F1: 0.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Train fine-tuned model on full training set"
      ],
      "metadata": {
        "id": "z35RSAb46oAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(model, test_loader, device)\n",
        "print(\"\\nFinal Test Results:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmQvBlnf6Y4b",
        "outputId": "21985a04-b8f4-42df-956d-b23b18ceece1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 13/13 [00:00<00:00, 17.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Results:\n",
            "Accuracy: 0.8950, Precision: 0.8010, Recall: 0.8950, F1: 0.8454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Train fine-tuned model with K-fold"
      ],
      "metadata": {
        "id": "Y-erDe8A6xOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print cross-validation results\n",
        "print(\"\\nCross-validation results:\")\n",
        "print(f\"Mean Accuracy: {np.mean(cv_accuracies):.4f} (+/- {np.std(cv_accuracies):.4f})\")\n",
        "print(f\"Mean F1 Score: {np.mean(cv_f1_scores):.4f} (+/- {np.std(cv_f1_scores):.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sp0GiJg6jO1",
        "outputId": "918ebe3a-921c-4218-bbae-2555cb5a5fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-validation results:\n",
            "Mean Accuracy: 0.9060 (+/- 0.0232)\n",
            "Mean F1 Score: 0.8627 (+/- 0.0338)\n"
          ]
        }
      ]
    }
  ]
}